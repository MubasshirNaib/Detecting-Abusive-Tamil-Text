{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10410904,"sourceType":"datasetVersion","datasetId":6451904},{"sourceId":10414122,"sourceType":"datasetVersion","datasetId":6454274}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n# # Import required libraries\n# import pandas as pd\n# from sklearn.model_selection import train_test_split\n# from sklearn.feature_extraction.text import TfidfVectorizer\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.metrics import classification_report, accuracy_score\n\n# # Load the training dataset\n# train_data = pd.read_csv(\"/kaggle/input/train-data/AWT_train.csv\")\n\n# # Check for missing values in the \"Class\" column\n# print(\"Number of missing values in 'Class' before cleaning:\", train_data[\"Class\"].isna().sum())\n\n# # Drop rows with missing values in the \"Class\" column\n# train_data = train_data.dropna(subset=[\"Class\"])\n\n# # Map non-numeric labels to integers\n# label_mapping = {\"Non-Abusive\": 0, \"Abusive\": 1}\n# train_data[\"Class\"] = train_data[\"Class\"].map(label_mapping)\n\n# # Ensure there are no NaN values after mapping\n# train_data = train_data.dropna(subset=[\"Class\"])\n# print(\"Number of missing values in 'Class' after mapping and cleaning:\", train_data[\"Class\"].isna().sum())\n\n# # Split the training data into train and validation sets\n# X_train, X_val, y_train, y_val = train_test_split(\n#     train_data[\"Text\"], train_data[\"Class\"], test_size=0.2, random_state=42\n# )\n\n# # Ensure there are no NaN values in training and validation labels\n# print(\"NaN in y_train:\", pd.isna(y_train).sum())\n# print(\"NaN in y_val:\", pd.isna(y_val).sum())\n\n# # Text Vectorization using TF-IDF\n# vectorizer = TfidfVectorizer(max_features=5000)\n# X_train_tfidf = vectorizer.fit_transform(X_train)\n# X_val_tfidf = vectorizer.transform(X_val)\n\n# # Initialize and Train the Logistic Regression Model\n# model = LogisticRegression()\n# model.fit(X_train_tfidf, y_train)\n\n# # Evaluate on the validation set\n# y_val_pred = model.predict(X_val_tfidf)\n# print(\"Validation Classification Report:\\n\", classification_report(y_val, y_val_pred))\n# print(\"Validation Accuracy:\", accuracy_score(y_val, y_val_pred))\n\n# # Load the test dataset\n# test_data = pd.read_csv(\"/kaggle/input/test-data/test_predictions.csv\")\n\n# # Vectorize the test data\n# X_test_tfidf = vectorizer.transform(test_data[\"Text\"])\n\n# # Make predictions on the test dataset\n# test_predictions = model.predict(X_test_tfidf)\n\n# # Convert predictions back to original label names\n# reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n# test_predictions_labels = [reverse_label_mapping[pred] for pred in test_predictions]\n\n# # Save predictions to a CSV file\n# output = pd.DataFrame({\"Text\": test_data[\"Text\"], \"Predicted_Class\": test_predictions_labels})\n# output.to_csv(\"test_predictions.csv\", index=False)\n# print(\"Predictions saved to 'test_predictions.csv'.\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-09T09:57:58.199627Z","iopub.execute_input":"2025-01-09T09:57:58.199875Z","iopub.status.idle":"2025-01-09T09:57:59.384015Z","shell.execute_reply.started":"2025-01-09T09:57:58.199849Z","shell.execute_reply":"2025-01-09T09:57:59.383080Z"}},"outputs":[{"name":"stdout","text":"Number of missing values in 'Class' before cleaning: 0\nNumber of missing values in 'Class' after mapping and cleaning: 0\nNaN in y_train: 0\nNaN in y_val: 0\nValidation Classification Report:\n               precision    recall  f1-score   support\n\n         0.0       0.67      0.72      0.70       281\n         1.0       0.70      0.64      0.67       277\n\n    accuracy                           0.68       558\n   macro avg       0.68      0.68      0.68       558\nweighted avg       0.68      0.68      0.68       558\n\nValidation Accuracy: 0.6827956989247311\nPredictions saved to 'test_predictions.csv'.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"pip install transformers datasets\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T10:00:10.721288Z","iopub.execute_input":"2025-01-09T10:00:10.721602Z","iopub.status.idle":"2025-01-09T10:00:15.160523Z","shell.execute_reply.started":"2025-01-09T10:00:10.721578Z","shell.execute_reply":"2025-01-09T10:00:15.159493Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (18.1.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.6.1)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install evaluate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T10:04:46.230428Z","iopub.execute_input":"2025-01-09T10:04:46.230770Z","iopub.status.idle":"2025-01-09T10:04:49.711524Z","shell.execute_reply.started":"2025-01-09T10:04:46.230744Z","shell.execute_reply":"2025-01-09T10:04:49.710284Z"}},"outputs":[{"name":"stderr","text":"/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.2.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.5)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.24.7)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.16.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.10.5)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.8.30)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.2)\nRequirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.11.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# # Import required libraries\n# import pandas as pd\n# from datasets import Dataset\n# from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n# import torch\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import accuracy_score, precision_score, recall_score\n\n# # Load the dataset\n# train_data = pd.read_csv(\"/kaggle/input/train-data/AWT_train.csv\")\n# test_data = pd.read_csv(\"/kaggle/input/test1-1-data/AWT_test_without_labels.csv\")\n\n# # Map the labels\n# label_mapping = {\"Non-Abusive\": 0, \"Abusive\": 1}\n# train_data[\"Class\"] = train_data[\"Class\"].map(label_mapping)\n\n# # Ensure no missing values\n# train_data = train_data.dropna(subset=[\"Class\"])\n\n# # Split the dataset into training and validation sets\n# train_texts, val_texts, train_labels, val_labels = train_test_split(\n#     train_data[\"Text\"], train_data[\"Class\"], test_size=0.1, random_state=42\n# )\n\n# # Initialize the tokenizer\n# tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n\n# # Tokenize the dataset\n# def preprocess_function(texts, labels):\n#     tokenized = tokenizer(list(texts), padding=\"max_length\", truncation=True, max_length=128)\n#     tokenized[\"labels\"] = labels\n#     return tokenized\n\n# train_encodings = preprocess_function(train_texts, list(train_labels))\n# val_encodings = preprocess_function(val_texts, list(val_labels))\n\n# # Convert tokenized data to PyTorch datasets\n# class CustomDataset(torch.utils.data.Dataset):\n#     def __init__(self, encodings):\n#         self.encodings = encodings\n\n#     def __len__(self):\n#         return len(self.encodings[\"input_ids\"])\n\n#     def __getitem__(self, idx):\n#         return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n\n# train_dataset = CustomDataset(train_encodings)\n# val_dataset = CustomDataset(val_encodings)\n\n# # Initialize the model\n# model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=1)\n\n# # Define the compute_metrics function\n# def compute_metrics(pred):\n#     predictions, labels = pred\n#     preds = (torch.sigmoid(torch.tensor(predictions)) > 0.5).int()\n#     labels = torch.tensor(labels).int()\n    \n#     accuracy = accuracy_score(labels, preds)\n#     precision = precision_score(labels, preds)\n#     recall = recall_score(labels, preds)\n    \n#     return {\n#         \"accuracy\": accuracy,\n#         \"precision\": precision,\n#         \"recall\": recall,\n#     }\n\n# # Define the Trainer arguments\n# training_args = TrainingArguments(\n#     output_dir=\"./results\",               # Output directory\n#     eval_strategy=\"epoch\",               # Evaluate every epoch\n#     save_strategy=\"epoch\",               # Save every epoch\n#     learning_rate=5e-5,                  # Learning rate\n#     per_device_train_batch_size=16,      # Batch size for training\n#     per_device_eval_batch_size=16,       # Batch size for evaluation\n#     num_train_epochs=3,                  # Number of epochs\n#     weight_decay=0.01,                   # Weight decay\n#     logging_dir=\"./logs\",                # Logging directory\n#     logging_steps=10,                    # Log every 10 steps\n#     report_to=\"none\",                    # Disable W&B logging\n# )\n\n# # Define the Trainer\n# trainer = Trainer(\n#     model=model,\n#     args=training_args,\n#     train_dataset=train_dataset,\n#     eval_dataset=val_dataset,  # Add validation dataset\n#     tokenizer=tokenizer,\n#     compute_metrics=compute_metrics,  # Add the metrics function\n# )\n\n# # Train the model\n# trainer.train()\n\n# # Make predictions on the test set\n# test_encodings = preprocess_function(test_data[\"Text\"], [0] * len(test_data))\n# test_dataset = CustomDataset(test_encodings)\n\n# predictions = trainer.predict(test_dataset)\n# preds = torch.sigmoid(torch.tensor(predictions.predictions)).numpy()\n# pred_labels = (preds > 0.5).astype(int).flatten()\n\n# # Save predictions\n# test_data[\"Predicted_Class\"] = pred_labels\n# test_data[\"Predicted_Class\"] = test_data[\"Predicted_Class\"].map({0: \"Non-Abusive\", 1: \"Abusive\"})\n# test_data.to_csv(\"test_predictions.csv\", index=False)\n# print(\"Predictions saved to 'test_predictions.csv'.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-09T17:03:18.291389Z","iopub.execute_input":"2025-01-09T17:03:18.291742Z","iopub.status.idle":"2025-01-09T17:05:58.660845Z","shell.execute_reply.started":"2025-01-09T17:03:18.291718Z","shell.execute_reply":"2025-01-09T17:05:58.660145Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='237' max='237' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [237/237 02:33, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.250400</td>\n      <td>0.239480</td>\n      <td>0.483871</td>\n      <td>0.483871</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.202600</td>\n      <td>0.208212</td>\n      <td>0.483871</td>\n      <td>0.483871</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.176800</td>\n      <td>0.185809</td>\n      <td>0.483871</td>\n      <td>0.483871</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Predictions saved to 'test_predictions.csv'.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\nfrom transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\n\n# Load the dataset\ntrain_data = pd.read_csv(\"/kaggle/input/train-data/AWT_train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/test1-1-data/AWT_test_without_labels.csv\")\n\n# Map the labels\nlabel_mapping = {\"Non-Abusive\": 0, \"Abusive\": 1}\ntrain_data[\"Class\"] = train_data[\"Class\"].map(label_mapping)\n\n# Ensure no missing values\ntrain_data = train_data.dropna(subset=[\"Class\"])\n\n# Split the dataset into training and validation sets\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    train_data[\"Text\"], train_data[\"Class\"], test_size=0.2, random_state=42\n)\n\n# Initialize the tokenizer\ntokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-multilingual-cased\")\n\n# Tokenize the dataset\ndef preprocess_function(texts, labels):\n    tokenized = tokenizer(list(texts), padding=\"max_length\", truncation=True, max_length=128)\n    tokenized[\"labels\"] = labels\n    return tokenized\n\ntrain_encodings = preprocess_function(train_texts, list(train_labels))\nval_encodings = preprocess_function(val_texts, list(val_labels))\n\n# Convert tokenized data to PyTorch datasets\nclass CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __len__(self):\n        return len(self.encodings[\"input_ids\"])\n\n    def __getitem__(self, idx):\n        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n\ntrain_dataset = CustomDataset(train_encodings)\nval_dataset = CustomDataset(val_encodings)\n\n# Initialize the model\nmodel = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-multilingual-cased\", num_labels=1)\n\n# Define the compute_metrics function\ndef compute_metrics(pred):\n    predictions, labels = pred\n    preds = (torch.sigmoid(torch.tensor(predictions)) > 0.5).int()\n    labels = torch.tensor(labels).int()\n    \n    accuracy = accuracy_score(labels, preds)\n    precision = precision_score(labels, preds)\n    recall = recall_score(labels, preds)\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0  # F1 Score\n    \n    return {\n        \"accuracy\": accuracy,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1,  # F1 Score\n    }\n\n# Define the Trainer arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",               # Output directory\n    eval_strategy=\"epoch\",                # Evaluate every epoch\n    save_strategy=\"epoch\",                # Save every epoch\n    learning_rate=5e-5,                   # Learning rate\n    per_device_train_batch_size=16,       # Batch size for training\n    per_device_eval_batch_size=16,        # Batch size for evaluation\n    num_train_epochs=7,                   # Number of epochs\n    weight_decay=0.003,                    # Weight decay\n    logging_dir=\"./logs\",                 # Logging directory\n    logging_steps=10,                     # Log every 10 steps\n    report_to=\"none\",                     # Disable W&B logging\n)\n\n# Define the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,  # Add validation dataset\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,  # Add the metrics function\n)\n\n# Train the model\ntrainer.train()\n\n# Make predictions on the test set\ntest_encodings = preprocess_function(test_data[\"Text\"], [0] * len(test_data))\ntest_dataset = CustomDataset(test_encodings)\n\npredictions = trainer.predict(test_dataset)\npreds = torch.sigmoid(torch.tensor(predictions.predictions)).numpy()\npred_labels = (preds > 0.5).astype(int).flatten()\n\n# Save predictions\ntest_data[\"Predicted_Class\"] = pred_labels\ntest_data[\"Predicted_Class\"] = test_data[\"Predicted_Class\"].map({0: \"Non-Abusive\", 1: \"Abusive\"})\ntest_data.to_csv(\"test_predictions_distilbert.csv\", index=False)\nprint(\"Predictions saved to 'test_predictions_distilbert.csv'.\") \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T15:50:00.127129Z","iopub.execute_input":"2025-01-27T15:50:00.127480Z","iopub.status.idle":"2025-01-27T15:53:45.929979Z","shell.execute_reply.started":"2025-01-27T15:50:00.127451Z","shell.execute_reply":"2025-01-27T15:53:45.929208Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nSome weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='490' max='490' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [490/490 03:40, Epoch 7/7]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.215100</td>\n      <td>0.189506</td>\n      <td>0.496416</td>\n      <td>0.496416</td>\n      <td>1.000000</td>\n      <td>0.663473</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.198000</td>\n      <td>0.179258</td>\n      <td>0.496416</td>\n      <td>0.496416</td>\n      <td>1.000000</td>\n      <td>0.663473</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.155500</td>\n      <td>0.167940</td>\n      <td>0.602151</td>\n      <td>0.555781</td>\n      <td>0.989170</td>\n      <td>0.711688</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.084000</td>\n      <td>0.179847</td>\n      <td>0.645161</td>\n      <td>0.586433</td>\n      <td>0.967509</td>\n      <td>0.730245</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.052500</td>\n      <td>0.195791</td>\n      <td>0.664875</td>\n      <td>0.605140</td>\n      <td>0.935018</td>\n      <td>0.734752</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.034700</td>\n      <td>0.196351</td>\n      <td>0.627240</td>\n      <td>0.575824</td>\n      <td>0.945848</td>\n      <td>0.715847</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.027500</td>\n      <td>0.197740</td>\n      <td>0.616487</td>\n      <td>0.569536</td>\n      <td>0.931408</td>\n      <td>0.706849</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Predictions saved to 'test_predictions_distilbert.csv'.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\n\n# Load the dataset\ntrain_data = pd.read_csv(\"/kaggle/input/train-data/AWT_train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/test1-1-data/AWT_test_without_labels.csv\")\n\n# Map the labels\nlabel_mapping = {\"Non-Abusive\": 0, \"Abusive\": 1}\ntrain_data[\"Class\"] = train_data[\"Class\"].map(label_mapping)\n\n# Ensure no missing values\ntrain_data = train_data.dropna(subset=[\"Class\"])\n\n# Split the dataset into training and validation sets\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    train_data[\"Text\"], train_data[\"Class\"], test_size=0.1, random_state=42\n)\n\n# Initialize the tokenizer\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n\n# Tokenize the dataset\ndef preprocess_function(texts, labels):\n    tokenized = tokenizer(list(texts), padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n    tokenized[\"labels\"] = labels\n    return tokenized\n\ntrain_encodings = preprocess_function(train_texts, list(train_labels))\nval_encodings = preprocess_function(val_texts, list(val_labels))\n\n# Convert tokenized data to PyTorch datasets\nclass CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __len__(self):\n        return len(self.encodings[\"input_ids\"])\n\n    def __getitem__(self, idx):\n        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n\ntrain_dataset = CustomDataset(train_encodings)\nval_dataset = CustomDataset(val_encodings)\n\n# Initialize the model\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=1)\n\n# Define the compute_metrics function\ndef compute_metrics(pred):\n    predictions, labels = pred\n    preds = (torch.sigmoid(torch.tensor(predictions)) > 0.5).int()\n    labels = torch.tensor(labels).int()\n    \n    accuracy = accuracy_score(labels, preds)\n    precision = precision_score(labels, preds)\n    recall = recall_score(labels, preds)\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n    \n    return {\n        \"accuracy\": accuracy,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1,\n    }\n\n# Define the TrainingArguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",               # Output directory\n    eval_strategy=\"epoch\",                # Evaluate every epoch\n    save_strategy=\"epoch\",                # Save every epoch\n    learning_rate=5e-5,                   # Learning rate\n    per_device_train_batch_size=16,       # Batch size for training\n    per_device_eval_batch_size=16,        # Batch size for evaluation\n    num_train_epochs=7,                   # Number of epochs\n    weight_decay=0.003,                   # Weight decay\n    logging_dir=\"./logs\",                 # Logging directory\n    logging_steps=10,                     # Log every 10 steps\n    report_to=\"none\",                     # Disable W&B logging\n)\n\n# Define the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\n# Train the model\ntrainer.train()\n\n# Make predictions on the test set\ntest_encodings = preprocess_function(test_data[\"Text\"], [0] * len(test_data))\ntest_dataset = CustomDataset(test_encodings)\n\npredictions = trainer.predict(test_dataset)\npreds = torch.sigmoid(torch.tensor(predictions.predictions)).numpy()\npred_labels = (preds > 0.5).astype(int).flatten()\n\n# Save predictions with only \"Id\" and \"Labels\"\ntest_data[\"Predicted_Class\"] = pred_labels\ntest_data[\"Predicted_Class\"] = test_data[\"Predicted_Class\"].map({0: \"Non-Abusive\", 1: \"Abusive\"})\ntest_data.to_csv(\"test_predictions_bert.csv\", index=False)\nprint(\"Predictions saved to 'test_predictions_bert.csv'.\") ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T10:26:04.651233Z","iopub.execute_input":"2025-01-27T10:26:04.651540Z","iopub.status.idle":"2025-01-27T10:32:16.022689Z","shell.execute_reply.started":"2025-01-27T10:26:04.651517Z","shell.execute_reply":"2025-01-27T10:32:16.022002Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-11-bac636967b73>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='553' max='553' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [553/553 06:03, Epoch 7/7]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.203900</td>\n      <td>0.218733</td>\n      <td>0.483871</td>\n      <td>0.483871</td>\n      <td>1.000000</td>\n      <td>0.652174</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.172200</td>\n      <td>0.180378</td>\n      <td>0.505376</td>\n      <td>0.494505</td>\n      <td>1.000000</td>\n      <td>0.661765</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.126100</td>\n      <td>0.204632</td>\n      <td>0.526882</td>\n      <td>0.505618</td>\n      <td>1.000000</td>\n      <td>0.671642</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.104000</td>\n      <td>0.203772</td>\n      <td>0.709677</td>\n      <td>0.643617</td>\n      <td>0.896296</td>\n      <td>0.749226</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.057600</td>\n      <td>0.229401</td>\n      <td>0.706093</td>\n      <td>0.634518</td>\n      <td>0.925926</td>\n      <td>0.753012</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.056000</td>\n      <td>0.222159</td>\n      <td>0.483871</td>\n      <td>0.483871</td>\n      <td>1.000000</td>\n      <td>0.652174</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.020800</td>\n      <td>0.237181</td>\n      <td>0.702509</td>\n      <td>0.639785</td>\n      <td>0.881481</td>\n      <td>0.741433</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"<ipython-input-11-bac636967b73>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n<ipython-input-11-bac636967b73>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n<ipython-input-11-bac636967b73>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n<ipython-input-11-bac636967b73>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n<ipython-input-11-bac636967b73>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n<ipython-input-11-bac636967b73>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n<ipython-input-11-bac636967b73>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n<ipython-input-11-bac636967b73>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Predictions saved to 'test_predictions_bert.csv'.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# import pandas as pd\n\n# # Load the predictions\n# test_predictions = pd.read_csv(\"test_predictions_bert.csv\")\n\n# # Create the file with only 'id' and 'Predicted_Class'\n# prediction_file = test_predictions[['id', 'Predicted_Class']]\n\n# # Save the file\n# prediction_file.to_csv(\"test_predictions_bert_only_id_labels.csv\", index=False)\n\n# print(\"Prediction file with 'id' and 'Predicted_Class' saved as 'test_predictions_bert_only_id_labels.csv'.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T15:45:13.956637Z","iopub.execute_input":"2025-01-10T15:45:13.957128Z","iopub.status.idle":"2025-01-10T15:45:14.012451Z","shell.execute_reply.started":"2025-01-10T15:45:13.957083Z","shell.execute_reply":"2025-01-10T15:45:14.011265Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-c5e89fc0c0a5>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtest_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test_predictions_bert.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Create the file with only 'id' and 'Predicted_Class'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1722\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1723\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1724\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1725\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;31m# Fail here loudly instead of in cython after reading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pyarrow\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"],"ename":"EmptyDataError","evalue":"No columns to parse from file","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"# import pandas as pd\n# from datasets import Dataset\n# from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n# import torch\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import accuracy_score, precision_score, recall_score\n\n# # Load the dataset\n# train_data = pd.read_csv(\"/kaggle/input/train-data/AWT_train.csv\")\n# test_data = pd.read_csv(\"/kaggle/input/test1-1-data/AWT_test_without_labels.csv\")\n\n# # Map the labels\n# label_mapping = {\"Non-Abusive\": 0, \"Abusive\": 1}\n# train_data[\"Class\"] = train_data[\"Class\"].map(label_mapping)\n\n# # Ensure no missing values\n# train_data = train_data.dropna(subset=[\"Class\"])\n\n# # Split the dataset into training and validation sets\n# train_texts, val_texts, train_labels, val_labels = train_test_split(\n#     train_data[\"Text\"], train_data[\"Class\"], test_size=0.1, random_state=42\n# )\n\n# # Initialize the tokenizer\n# tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n\n# # Tokenize the dataset\n# def preprocess_function(texts, labels):\n#     tokenized = tokenizer(list(texts), padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n#     tokenized[\"labels\"] = labels\n#     return tokenized\n\n# train_encodings = preprocess_function(train_texts, list(train_labels))\n# val_encodings = preprocess_function(val_texts, list(val_labels))\n\n# # Convert tokenized data to PyTorch datasets\n# class CustomDataset(torch.utils.data.Dataset):\n#     def __init__(self, encodings):\n#         self.encodings = encodings\n\n#     def __len__(self):\n#         return len(self.encodings[\"input_ids\"])\n\n#     def __getitem__(self, idx):\n#         return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n\n# train_dataset = CustomDataset(train_encodings)\n# val_dataset = CustomDataset(val_encodings)\n\n# # Initialize the model\n# model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=1)\n\n# # Define the compute_metrics function\n# def compute_metrics(pred):\n#     predictions, labels = pred\n#     preds = (torch.sigmoid(torch.tensor(predictions)) > 0.5).int()\n#     labels = torch.tensor(labels).int()\n    \n#     accuracy = accuracy_score(labels, preds)\n#     precision = precision_score(labels, preds)\n#     recall = recall_score(labels, preds)\n#     f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0  # F1 Score\n    \n#     return {\n#         \"accuracy\": accuracy,\n#         \"precision\": precision,\n#         \"recall\": recall,\n#         \"f1\": f1,  # F1 Score\n#     }\n\n# # Define the TrainingArguments\n# training_args = TrainingArguments(\n#     output_dir=\"./results\",               # Output directory\n#     eval_strategy=\"epoch\",                # Evaluate every epoch\n#     save_strategy=\"epoch\",                # Save every epoch\n#     learning_rate=5e-5,                   # Learning rate (try adjusting this)\n#     per_device_train_batch_size=16,       # Batch size for training\n#     per_device_eval_batch_size=16,        # Batch size for evaluation\n#     num_train_epochs=7,                   # Number of epochs\n#     weight_decay=0.003,                    # Weight decay\n#     logging_dir=\"./logs\",                 # Logging directory\n#     logging_steps=10,                     # Log every 10 steps\n#     report_to=\"none\",                     # Disable W&B logging\n# )\n\n# # Define the Trainer\n# trainer = Trainer(\n#     model=model,\n#     args=training_args,\n#     train_dataset=train_dataset,\n#     eval_dataset=val_dataset,  # Add validation dataset\n#     tokenizer=tokenizer,\n#     compute_metrics=compute_metrics,  # Add the metrics function\n# )\n\n# # Train the model\n# trainer.train()\n\n# # Make predictions on the test set\n# test_encodings = preprocess_function(test_data[\"Text\"], [0] * len(test_data))\n# test_dataset = CustomDataset(test_encodings)\n\n# predictions = trainer.predict(test_dataset)\n# preds = torch.sigmoid(torch.tensor(predictions.predictions)).numpy()\n# pred_labels = (preds > 0.5).astype(int).flatten()\n\n# # Save predictions\n# test_data[\"Labels\"] = pred_labels\n# test_data[\"Labels\"] = test_data[\"Labels\"].map({0: \"Non-Abusive\", 1: \"Abusive\"})\n# #test_data.to_csv(\"test_predictions_bert.csv\", index=False)\n# #print(\"Predictions saved to 'test_predictions_bert.csv'.\") \n# # Load the predictions\n# #test_predictions = pd.read_csv(\"test_predictions_bert.csv\")\n\n# # Create the file with only 'id' and 'Predicted_Class'\n# prediction_file = test_data[['id', 'Labels']]\n\n# # Save the file\n# prediction_file.to_csv(\"test_predictions_bert_only_id_labels1.csv\", index=False)\n\n# print(\"Prediction file with 'id' and 'Predicted_Class' saved as 'test_predictions_bert_only_id_labels.csv'.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T10:16:03.964461Z","iopub.execute_input":"2025-01-27T10:16:03.964832Z","iopub.status.idle":"2025-01-27T10:22:17.836326Z","shell.execute_reply.started":"2025-01-27T10:16:03.964800Z","shell.execute_reply":"2025-01-27T10:22:17.835564Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\nSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-10-a78e34157025>:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='553' max='553' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [553/553 06:05, Epoch 7/7]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.195500</td>\n      <td>0.158613</td>\n      <td>0.483871</td>\n      <td>0.483871</td>\n      <td>1.000000</td>\n      <td>0.652174</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.165400</td>\n      <td>0.179616</td>\n      <td>0.483871</td>\n      <td>0.483871</td>\n      <td>1.000000</td>\n      <td>0.652174</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.107900</td>\n      <td>0.186692</td>\n      <td>0.483871</td>\n      <td>0.483871</td>\n      <td>1.000000</td>\n      <td>0.652174</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.076500</td>\n      <td>0.207696</td>\n      <td>0.491039</td>\n      <td>0.487365</td>\n      <td>1.000000</td>\n      <td>0.655340</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.063500</td>\n      <td>0.200183</td>\n      <td>0.483871</td>\n      <td>0.483871</td>\n      <td>1.000000</td>\n      <td>0.652174</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.039900</td>\n      <td>0.229876</td>\n      <td>0.483871</td>\n      <td>0.483871</td>\n      <td>1.000000</td>\n      <td>0.652174</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.014900</td>\n      <td>0.234219</td>\n      <td>0.548387</td>\n      <td>0.517510</td>\n      <td>0.985185</td>\n      <td>0.678571</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"<ipython-input-10-a78e34157025>:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n<ipython-input-10-a78e34157025>:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n<ipython-input-10-a78e34157025>:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n<ipython-input-10-a78e34157025>:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n<ipython-input-10-a78e34157025>:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n<ipython-input-10-a78e34157025>:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n<ipython-input-10-a78e34157025>:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n<ipython-input-10-a78e34157025>:45: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Prediction file with 'id' and 'Predicted_Class' saved as 'test_predictions_bert_only_id_labels.csv'.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# import pandas as pd\n\n# # Load the predictions\n# test_predictions = pd.read_csv(\"test_predictions_bert.csv\")\n\n# # Create the file with only 'id' and 'Predicted_Class'\n# prediction_file = test_predictions[['id', 'Predicted_Class']]\n\n# # Save the file\n# prediction_file.to_csv(\"test_predictions_bert_only_id_labels.csv\", index=False)\n\n# print(\"Prediction file with 'id' and 'Predicted_Class' saved as 'test_predictions_bert_only_id_labels.csv'.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-10T15:43:04.978947Z","iopub.execute_input":"2025-01-10T15:43:04.979251Z","iopub.status.idle":"2025-01-10T15:43:05.020021Z","shell.execute_reply.started":"2025-01-10T15:43:04.979229Z","shell.execute_reply":"2025-01-10T15:43:05.018866Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-c5e89fc0c0a5>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Load the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtest_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test_predictions_bert.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Create the file with only 'id' and 'Predicted_Class'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1722\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1723\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1724\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1725\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;31m# Fail here loudly instead of in cython after reading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pyarrow\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"],"ename":"EmptyDataError","evalue":"No columns to parse from file","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\nfrom transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, Trainer, TrainingArguments\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\n\n# Load the dataset\ntrain_data = pd.read_csv(\"/kaggle/input/train-data/AWT_train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/test1-1-data/AWT_test_without_labels.csv\")\n\n# Map the labels\nlabel_mapping = {\"Non-Abusive\": 0, \"Abusive\": 1}\ntrain_data[\"Class\"] = train_data[\"Class\"].map(label_mapping)\n\n# Ensure no missing values\ntrain_data = train_data.dropna(subset=[\"Class\"])\n\n# Split the dataset into training and validation sets\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    train_data[\"Text\"], train_data[\"Class\"], test_size=0.1, random_state=42\n)\n\n# Initialize the tokenizer\ntokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n\n# Tokenize the dataset\ndef preprocess_function(texts, labels):\n    tokenized = tokenizer(list(texts), padding=\"max_length\", truncation=True, max_length=128)\n    tokenized[\"labels\"] = labels\n    return tokenized\n\ntrain_encodings = preprocess_function(train_texts, list(train_labels))\nval_encodings = preprocess_function(val_texts, list(val_labels))\n\n# Convert tokenized data to PyTorch datasets\nclass CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __len__(self):\n        return len(self.encodings[\"input_ids\"])\n\n    def __getitem__(self, idx):\n        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n\ntrain_dataset = CustomDataset(train_encodings)\nval_dataset = CustomDataset(val_encodings)\n\n# Initialize the model\nmodel = XLMRobertaForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=1)\n\n# Define the compute_metrics function\ndef compute_metrics(pred):\n    predictions, labels = pred\n    preds = (torch.sigmoid(torch.tensor(predictions)) > 0.5).int()\n    labels = torch.tensor(labels).int()\n    \n    accuracy = accuracy_score(labels, preds)\n    precision = precision_score(labels, preds)\n    recall = recall_score(labels, preds)\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0  # F1 Score\n    \n    return {\n        \"accuracy\": accuracy,\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1,  # F1 Score\n    }\n\n# Define the Trainer arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",               # Output directory\n    eval_strategy=\"epoch\",                # Evaluate every epoch\n    save_strategy=\"epoch\",                # Save every epoch\n    learning_rate=5e-5,                   # Learning rate\n    per_device_train_batch_size=16,       # Batch size for training\n    per_device_eval_batch_size=16,        # Batch size for evaluation\n    num_train_epochs=3,                   # Number of epochs\n    weight_decay=0.003,                    # Weight decay\n    logging_dir=\"./logs\",                 # Logging directory\n    logging_steps=10,                     # Log every 10 steps\n    report_to=\"none\",                     # Disable W&B logging\n)\n\n# Define the Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,  # Add validation dataset\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,  # Add the metrics function\n)\n\n# Train the model\ntrainer.train()\n\n# Make predictions on the test set\ntest_encodings = preprocess_function(test_data[\"Text\"], [0] * len(test_data))\ntest_dataset = CustomDataset(test_encodings)\n\npredictions = trainer.predict(test_dataset)\npreds = torch.sigmoid(torch.tensor(predictions.predictions)).numpy()\npred_labels = (preds > 0.5).astype(int).flatten()\n\n# Save predictions\ntest_data[\"Predicted_Class\"] = pred_labels\ntest_data[\"Predicted_Class\"] = test_data[\"Predicted_Class\"].map({0: \"Non-Abusive\", 1: \"Abusive\"})\ntest_data.to_csv(\"test_predictions_xlmr.csv\", index=False)\nprint(\"Predictions saved to 'test_predictions_xlmr.csv'.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T13:33:14.832792Z","iopub.execute_input":"2025-01-27T13:33:14.833100Z","iopub.status.idle":"2025-01-27T13:37:06.126002Z","shell.execute_reply.started":"2025-01-27T13:33:14.833075Z","shell.execute_reply":"2025-01-27T13:37:06.125085Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba8e810f6e234c2fae6f3cc2a4a94c84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30c93802d0994ae8846a47200c1de1e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c698fb3177a4ef7b0ff23b30300716e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c18dd8b6805484fa67132a1100ffed8"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdd601cd177942e5ba53d33c67109fa4"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='237' max='237' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [237/237 03:05, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.260800</td>\n      <td>0.225414</td>\n      <td>0.483871</td>\n      <td>0.483871</td>\n      <td>1.000000</td>\n      <td>0.652174</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.258700</td>\n      <td>0.249904</td>\n      <td>0.483871</td>\n      <td>0.483871</td>\n      <td>1.000000</td>\n      <td>0.652174</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.260100</td>\n      <td>0.249768</td>\n      <td>0.483871</td>\n      <td>0.483871</td>\n      <td>1.000000</td>\n      <td>0.652174</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Predictions saved to 'test_predictions_xlmr.csv'.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# # Import required libraries\n# import pandas as pd\n# from sklearn.model_selection import train_test_split\n# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.svm import SVC\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.naive_bayes import MultinomialNB\n# from sklearn.metrics import classification_report, accuracy_score\n\n# # Load the training dataset\n# train_data = pd.read_csv(\"/kaggle/input/train-data/AWT_train.csv\")\n\n# # Check for missing values in the \"Class\" column\n# print(\"Number of missing values in 'Class' before cleaning:\", train_data[\"Class\"].isna().sum())\n\n# # Drop rows with missing values in the \"Class\" column\n# train_data = train_data.dropna(subset=[\"Class\"])\n\n# # Map non-numeric labels to integers\n# label_mapping = {\"Non-Abusive\": 0, \"Abusive\": 1}\n# train_data[\"Class\"] = train_data[\"Class\"].map(label_mapping)\n\n# # Ensure there are no NaN values after mapping\n# train_data = train_data.dropna(subset=[\"Class\"])\n# print(\"Number of missing values in 'Class' after mapping and cleaning:\", train_data[\"Class\"].isna().sum())\n\n# # Split the training data into train and validation sets\n# X_train, X_val, y_train, y_val = train_test_split(\n#     train_data[\"Text\"], train_data[\"Class\"], test_size=0.2, random_state=42\n# )\n\n# # Ensure there are no NaN values in training and validation labels\n# print(\"NaN in y_train:\", pd.isna(y_train).sum())\n# print(\"NaN in y_val:\", pd.isna(y_val).sum())\n\n# # Define a function to train and evaluate models\n# def train_and_evaluate(models, feature_extractor, feature_name):\n#     # Transform text data using the feature extractor\n#     X_train_features = feature_extractor.fit_transform(X_train)\n#     X_val_features = feature_extractor.transform(X_val)\n    \n#     # Iterate over the models and train them\n#     for model_name, model in models.items():\n#         print(f\"\\nTraining {model_name} with {feature_name} features...\")\n#         model.fit(X_train_features, y_train)\n#         y_val_pred = model.predict(X_val_features)\n        \n#         # Evaluate the model\n#         print(f\"\\nValidation Results for {model_name} ({feature_name}):\")\n#         print(\"Classification Report:\\n\", classification_report(y_val, y_val_pred))\n#         print(\"Accuracy:\", accuracy_score(y_val, y_val_pred))\n\n# # Define models to train\n# models = {\n#     \"Logistic Regression\": LogisticRegression(),\n#     \"SVM\": SVC(),\n#     \"Random Forest\": RandomForestClassifier(),\n#     \"Multinomial Naive Bayes\": MultinomialNB()\n# }\n\n# # Evaluate with TF-IDF features\n# print(\"\\nUsing TF-IDF features:\")\n# tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n# train_and_evaluate(models, tfidf_vectorizer, \"TF-IDF\")\n\n# # Evaluate with Bag of Words (BoW) features\n# print(\"\\nUsing Bag of Words (BoW) features:\")\n# bow_vectorizer = CountVectorizer(max_features=5000)\n# train_and_evaluate(models, bow_vectorizer, \"BoW\")\n\n# # Load the test dataset\n# test_data = pd.read_csv(\"/kaggle/input/test1-1-data/AWT_test_without_labels.csv\")\n\n# # Transform test data using the TF-IDF vectorizer\n# X_test_tfidf = tfidf_vectorizer.transform(test_data[\"Text\"])\n\n# # Make predictions on the test dataset using the best-performing model (e.g., Logistic Regression)\n# best_model = LogisticRegression()  # Replace with your chosen model if needed\n# best_model.fit(tfidf_vectorizer.fit_transform(X_train), y_train)  # Retrain on the full training data\n# test_predictions = best_model.predict(X_test_tfidf)\n\n# # Convert predictions back to original label names\n# reverse_label_mapping = {v: k for k, v in label_mapping.items()}\n# test_predictions_labels = [reverse_label_mapping[pred] for pred in test_predictions]\n\n# # Save predictions to a CSV file\n# output = pd.DataFrame({\"Text\": test_data[\"Text\"], \"Predicted_Class\": test_predictions_labels})\n# output.to_csv(\"test_predictions.csv\", index=False)\n# print(\"Predictions saved to 'test_predictions.csv'.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T13:39:18.506720Z","iopub.execute_input":"2025-01-27T13:39:18.507090Z","iopub.status.idle":"2025-01-27T13:39:22.694848Z","shell.execute_reply.started":"2025-01-27T13:39:18.507049Z","shell.execute_reply":"2025-01-27T13:39:22.694029Z"}},"outputs":[{"name":"stdout","text":"Number of missing values in 'Class' before cleaning: 0\nNumber of missing values in 'Class' after mapping and cleaning: 0\nNaN in y_train: 0\nNaN in y_val: 0\n\nUsing TF-IDF features:\n\nTraining Logistic Regression with TF-IDF features...\n\nValidation Results for Logistic Regression (TF-IDF):\nClassification Report:\n               precision    recall  f1-score   support\n\n         0.0       0.67      0.72      0.70       281\n         1.0       0.70      0.64      0.67       277\n\n    accuracy                           0.68       558\n   macro avg       0.68      0.68      0.68       558\nweighted avg       0.68      0.68      0.68       558\n\nAccuracy: 0.6827956989247311\n\nTraining SVM with TF-IDF features...\n\nValidation Results for SVM (TF-IDF):\nClassification Report:\n               precision    recall  f1-score   support\n\n         0.0       0.66      0.73      0.69       281\n         1.0       0.69      0.62      0.66       277\n\n    accuracy                           0.68       558\n   macro avg       0.68      0.68      0.67       558\nweighted avg       0.68      0.68      0.67       558\n\nAccuracy: 0.6756272401433692\n\nTraining Random Forest with TF-IDF features...\n\nValidation Results for Random Forest (TF-IDF):\nClassification Report:\n               precision    recall  f1-score   support\n\n         0.0       0.64      0.67      0.65       281\n         1.0       0.65      0.63      0.64       277\n\n    accuracy                           0.65       558\n   macro avg       0.65      0.65      0.65       558\nweighted avg       0.65      0.65      0.65       558\n\nAccuracy: 0.6469534050179212\n\nTraining Multinomial Naive Bayes with TF-IDF features...\n\nValidation Results for Multinomial Naive Bayes (TF-IDF):\nClassification Report:\n               precision    recall  f1-score   support\n\n         0.0       0.71      0.66      0.69       281\n         1.0       0.68      0.73      0.70       277\n\n    accuracy                           0.69       558\n   macro avg       0.69      0.69      0.69       558\nweighted avg       0.69      0.69      0.69       558\n\nAccuracy: 0.6935483870967742\n\nUsing Bag of Words (BoW) features:\n\nTraining Logistic Regression with BoW features...\n\nValidation Results for Logistic Regression (BoW):\nClassification Report:\n               precision    recall  f1-score   support\n\n         0.0       0.66      0.72      0.69       281\n         1.0       0.68      0.62      0.65       277\n\n    accuracy                           0.67       558\n   macro avg       0.67      0.67      0.67       558\nweighted avg       0.67      0.67      0.67       558\n\nAccuracy: 0.6702508960573477\n\nTraining SVM with BoW features...\n\nValidation Results for SVM (BoW):\nClassification Report:\n               precision    recall  f1-score   support\n\n         0.0       0.65      0.73      0.69       281\n         1.0       0.68      0.60      0.64       277\n\n    accuracy                           0.66       558\n   macro avg       0.67      0.66      0.66       558\nweighted avg       0.67      0.66      0.66       558\n\nAccuracy: 0.6648745519713262\n\nTraining Random Forest with BoW features...\n\nValidation Results for Random Forest (BoW):\nClassification Report:\n               precision    recall  f1-score   support\n\n         0.0       0.68      0.67      0.68       281\n         1.0       0.67      0.69      0.68       277\n\n    accuracy                           0.68       558\n   macro avg       0.68      0.68      0.68       558\nweighted avg       0.68      0.68      0.68       558\n\nAccuracy: 0.6774193548387096\n\nTraining Multinomial Naive Bayes with BoW features...\n\nValidation Results for Multinomial Naive Bayes (BoW):\nClassification Report:\n               precision    recall  f1-score   support\n\n         0.0       0.71      0.66      0.68       281\n         1.0       0.68      0.72      0.70       277\n\n    accuracy                           0.69       558\n   macro avg       0.69      0.69      0.69       558\nweighted avg       0.69      0.69      0.69       558\n\nAccuracy: 0.6899641577060932\nPredictions saved to 'test_predictions.csv'.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Import required libraries\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report, accuracy_score\n\n# Load the training dataset\ntrain_data = pd.read_csv(\"/kaggle/input/train-data/AWT_train.csv\")\n\n# Check for missing values in the \"Class\" column\nprint(\"Number of missing values in 'Class' before cleaning:\", train_data[\"Class\"].isna().sum())\n\n# Drop rows with missing values in the \"Class\" column\ntrain_data = train_data.dropna(subset=[\"Class\"])\n\n# Map non-numeric labels to integers\nlabel_mapping = {\"Non-Abusive\": 0, \"Abusive\": 1}\ntrain_data[\"Class\"] = train_data[\"Class\"].map(label_mapping)\n\n# Ensure there are no NaN values after mapping\ntrain_data = train_data.dropna(subset=[\"Class\"])\nprint(\"Number of missing values in 'Class' after mapping and cleaning:\", train_data[\"Class\"].isna().sum())\n\n# Split the training data into train and validation sets\nX_train, X_val, y_train, y_val = train_test_split(\n    train_data[\"Text\"], train_data[\"Class\"], test_size=0.2, random_state=42\n)\n\n# Ensure there are no NaN values in training and validation labels\nprint(\"NaN in y_train:\", pd.isna(y_train).sum())\nprint(\"NaN in y_val:\", pd.isna(y_val).sum())\n\n# Define a function to train and evaluate models\ndef train_and_evaluate(models, feature_extractor, feature_name):\n    # Transform text data using the feature extractor\n    X_train_features = feature_extractor.fit_transform(X_train)\n    X_val_features = feature_extractor.transform(X_val)\n    \n    # Iterate over the models and train them\n    for model_name, model in models.items():\n        print(f\"\\nTraining {model_name} with {feature_name} features...\")\n        model.fit(X_train_features, y_train)\n        y_val_pred = model.predict(X_val_features)\n        \n        # Evaluate the model\n        print(f\"\\nValidation Results for {model_name} ({feature_name}):\")\n        print(\"Classification Report:\\n\", classification_report(y_val, y_val_pred, digits=4))\n        accuracy = accuracy_score(y_val, y_val_pred)\n        print(f\"Accuracy: {accuracy:.4f}\")\n\n# Define models to train\nmodels = {\n    \"Logistic Regression\": LogisticRegression(),\n    \"SVM\": SVC(),\n    \"Random Forest\": RandomForestClassifier(),\n    \"Multinomial Naive Bayes\": MultinomialNB()\n}\n\n# Evaluate with TF-IDF features\nprint(\"\\nUsing TF-IDF features:\")\ntfidf_vectorizer = TfidfVectorizer(max_features=5000)\ntrain_and_evaluate(models, tfidf_vectorizer, \"TF-IDF\")\n\n# Evaluate with Bag of Words (BoW) features\nprint(\"\\nUsing Bag of Words (BoW) features:\")\nbow_vectorizer = CountVectorizer(max_features=5000)\ntrain_and_evaluate(models, bow_vectorizer, \"BoW\")\n\n# Load the test dataset\ntest_data = pd.read_csv(\"/kaggle/input/test1-1-data/AWT_test_without_labels.csv\")\n\n# Transform test data using the TF-IDF vectorizer\nX_test_tfidf = tfidf_vectorizer.transform(test_data[\"Text\"])\n\n# Make predictions on the test dataset using the best-performing model (e.g., Logistic Regression)\nbest_model = LogisticRegression()  # Replace with your chosen model if needed\nbest_model.fit(tfidf_vectorizer.fit_transform(X_train), y_train)  # Retrain on the full training data\ntest_predictions = best_model.predict(X_test_tfidf)\n\n# Convert predictions back to original label names\nreverse_label_mapping = {v: k for k, v in label_mapping.items()}\ntest_predictions_labels = [reverse_label_mapping[pred] for pred in test_predictions]\n\n# Save predictions to a CSV file\noutput = pd.DataFrame({\"Text\": test_data[\"Text\"], \"Predicted_Class\": test_predictions_labels})\noutput.to_csv(\"test_predictions.csv\", index=False)\nprint(\"Predictions saved to 'test_predictions.csv'.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T13:58:01.144486Z","iopub.execute_input":"2025-01-27T13:58:01.144793Z","iopub.status.idle":"2025-01-27T13:58:04.762868Z","shell.execute_reply.started":"2025-01-27T13:58:01.144770Z","shell.execute_reply":"2025-01-27T13:58:04.761800Z"}},"outputs":[{"name":"stdout","text":"Number of missing values in 'Class' before cleaning: 0\nNumber of missing values in 'Class' after mapping and cleaning: 0\nNaN in y_train: 0\nNaN in y_val: 0\n\nUsing TF-IDF features:\n\nTraining Logistic Regression with TF-IDF features...\n\nValidation Results for Logistic Regression (TF-IDF):\nClassification Report:\n               precision    recall  f1-score   support\n\n         0.0     0.6722    0.7224    0.6964       281\n         1.0     0.6953    0.6426    0.6679       277\n\n    accuracy                         0.6828       558\n   macro avg     0.6837    0.6825    0.6822       558\nweighted avg     0.6837    0.6828    0.6823       558\n\nAccuracy: 0.6828\n\nTraining SVM with TF-IDF features...\n\nValidation Results for SVM (TF-IDF):\nClassification Report:\n               precision    recall  f1-score   support\n\n         0.0     0.6613    0.7295    0.6937       281\n         1.0     0.6935    0.6209    0.6552       277\n\n    accuracy                         0.6756       558\n   macro avg     0.6774    0.6752    0.6745       558\nweighted avg     0.6773    0.6756    0.6746       558\n\nAccuracy: 0.6756\n\nTraining Random Forest with TF-IDF features...\n\nValidation Results for Random Forest (TF-IDF):\nClassification Report:\n               precision    recall  f1-score   support\n\n         0.0     0.6438    0.6690    0.6562       281\n         1.0     0.6504    0.6245    0.6372       277\n\n    accuracy                         0.6470       558\n   macro avg     0.6471    0.6468    0.6467       558\nweighted avg     0.6471    0.6470    0.6468       558\n\nAccuracy: 0.6470\n\nTraining Multinomial Naive Bayes with TF-IDF features...\n\nValidation Results for Multinomial Naive Bayes (TF-IDF):\nClassification Report:\n               precision    recall  f1-score   support\n\n         0.0     0.7099    0.6619    0.6851       281\n         1.0     0.6791    0.7256    0.7016       277\n\n    accuracy                         0.6935       558\n   macro avg     0.6945    0.6938    0.6933       558\nweighted avg     0.6946    0.6935    0.6933       558\n\nAccuracy: 0.6935\n\nUsing Bag of Words (BoW) features:\n\nTraining Logistic Regression with BoW features...\n\nValidation Results for Logistic Regression (BoW):\nClassification Report:\n               precision    recall  f1-score   support\n\n         0.0     0.6590    0.7153    0.6860       281\n         1.0     0.6838    0.6245    0.6528       277\n\n    accuracy                         0.6703       558\n   macro avg     0.6714    0.6699    0.6694       558\nweighted avg     0.6713    0.6703    0.6695       558\n\nAccuracy: 0.6703\n\nTraining SVM with BoW features...\n\nValidation Results for SVM (BoW):\nClassification Report:\n               precision    recall  f1-score   support\n\n         0.0     0.6497    0.7260    0.6857       281\n         1.0     0.6844    0.6029    0.6411       277\n\n    accuracy                         0.6649       558\n   macro avg     0.6671    0.6644    0.6634       558\nweighted avg     0.6669    0.6649    0.6636       558\n\nAccuracy: 0.6649\n\nTraining Random Forest with BoW features...\n\nValidation Results for Random Forest (BoW):\nClassification Report:\n               precision    recall  f1-score   support\n\n         0.0     0.6787    0.6690    0.6738       281\n         1.0     0.6690    0.6787    0.6738       277\n\n    accuracy                         0.6738       558\n   macro avg     0.6739    0.6739    0.6738       558\nweighted avg     0.6739    0.6738    0.6738       558\n\nAccuracy: 0.6738\n\nTraining Multinomial Naive Bayes with BoW features...\n\nValidation Results for Multinomial Naive Bayes (BoW):\nClassification Report:\n               precision    recall  f1-score   support\n\n         0.0     0.7061    0.6584    0.6814       281\n         1.0     0.6757    0.7220    0.6981       277\n\n    accuracy                         0.6900       558\n   macro avg     0.6909    0.6902    0.6897       558\nweighted avg     0.6910    0.6900    0.6897       558\n\nAccuracy: 0.6900\nPredictions saved to 'test_predictions.csv'.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# import pandas as pd\n# from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n# import torch\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import accuracy_score, precision_score, recall_score\n\n# # Load the dataset\n# train_data = pd.read_csv(\"/kaggle/input/train-data/AWT_train.csv\")\n# test_data = pd.read_csv(\"/kaggle/input/test1-1-data/AWT_test_without_labels.csv\")\n\n# # Map the labels\n# label_mapping = {\"Non-Abusive\": 0, \"Abusive\": 1}\n# train_data[\"Class\"] = train_data[\"Class\"].map(label_mapping)\n\n# # Ensure no missing values\n# train_data = train_data.dropna(subset=[\"Class\"])\n\n# # Split the dataset into training and validation sets\n# train_texts, val_texts, train_labels, val_labels = train_test_split(\n#     train_data[\"Text\"], train_data[\"Class\"], test_size=0.2, random_state=42\n# )\n\n# # Initialize the tokenizer\n# tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n\n# # Tokenize the dataset\n# def preprocess_function(texts, labels):\n#     tokenized = tokenizer(list(texts), padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n#     tokenized[\"labels\"] = labels\n#     return tokenized\n\n# train_encodings = preprocess_function(train_texts, list(train_labels))\n# val_encodings = preprocess_function(val_texts, list(val_labels))\n\n# # Convert tokenized data to PyTorch datasets\n# class CustomDataset(torch.utils.data.Dataset):\n#     def __init__(self, encodings):\n#         self.encodings = encodings\n\n#     def __len__(self):\n#         return len(self.encodings[\"input_ids\"])\n\n#     def __getitem__(self, idx):\n#         return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n\n# train_dataset = CustomDataset(train_encodings)\n# val_dataset = CustomDataset(val_encodings)\n\n# # Initialize the model\n# model = BertForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\", num_labels=1)\n\n# # Define the compute_metrics function\n# def compute_metrics(pred):\n#     predictions, labels = pred\n#     preds = (torch.sigmoid(torch.tensor(predictions)) > 0.5).int()\n#     labels = torch.tensor(labels).int()\n    \n#     accuracy = accuracy_score(labels, preds)\n#     precision = precision_score(labels, preds)\n#     recall = recall_score(labels, preds)\n#     f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n    \n#     return {\n#         \"accuracy\": accuracy,\n#         \"precision\": precision,\n#         \"recall\": recall,\n#         \"f1\": f1,\n#     }\n\n# # Define the TrainingArguments\n# training_args = TrainingArguments(\n#     output_dir=\"./results\",               # Output directory\n#     eval_strategy=\"epoch\",                # Evaluate every epoch\n#     save_strategy=\"epoch\",                # Save every epoch\n#     learning_rate=5e-5,                   # Learning rate\n#     per_device_train_batch_size=16,       # Batch size for training\n#     per_device_eval_batch_size=16,        # Batch size for evaluation\n#     num_train_epochs=7,                   # Number of epochs\n#     weight_decay=0.003,                   # Weight decay\n#     logging_dir=\"./logs\",                 # Logging directory\n#     logging_steps=10,                     # Log every 10 steps\n#     report_to=\"none\",                     # Disable W&B logging\n# )\n\n# # Define the Trainer\n# trainer = Trainer(\n#     model=model,\n#     args=training_args,\n#     train_dataset=train_dataset,\n#     eval_dataset=val_dataset,\n#     tokenizer=tokenizer,\n#     compute_metrics=compute_metrics,\n# )\n\n# # Train the model\n# trainer.train()\n\n# # Make predictions on the test set\n# test_encodings = preprocess_function(test_data[\"Text\"], [0] * len(test_data))\n# test_dataset = CustomDataset(test_encodings)\n\n# predictions = trainer.predict(test_dataset)\n# preds = torch.sigmoid(torch.tensor(predictions.predictions)).numpy()\n# pred_labels = (preds > 0.5).astype(int).flatten()\n\n# # Save predictions with only \"Id\" and \"Labels\"\n# test_data[\"Predicted_Class\"] = pred_labels\n# test_data[\"Predicted_Class\"] = test_data[\"Predicted_Class\"].map({0: \"Non-Abusive\", 1: \"Abusive\"})\n# test_data.to_csv(\"test_predictions_bert.csv\", index=False)\n# print(\"Predictions saved to 'test_predictions_bert.csv'.\") ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-27T15:55:52.016584Z","iopub.execute_input":"2025-01-27T15:55:52.016907Z","iopub.status.idle":"2025-01-27T16:01:52.114054Z","shell.execute_reply.started":"2025-01-27T15:55:52.016883Z","shell.execute_reply":"2025-01-27T16:01:52.113305Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a005bcaee8ec49b7afea4e9e4830e841"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60aa5921829f419eaf2f91cdd5ef9249"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1f971ab0aaf47a6879f84f827e6462a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"503919b7335b450d9b57e99662cbd059"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b246ffe922a45959ecfe0f7d7fe1f82"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n<ipython-input-5-53ce25c34657>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='490' max='490' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [490/490 05:48, Epoch 7/7]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.261500</td>\n      <td>0.250011</td>\n      <td>0.496416</td>\n      <td>0.496416</td>\n      <td>1.000000</td>\n      <td>0.663473</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.261800</td>\n      <td>0.256118</td>\n      <td>0.496416</td>\n      <td>0.496416</td>\n      <td>1.000000</td>\n      <td>0.663473</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.257800</td>\n      <td>0.256510</td>\n      <td>0.496416</td>\n      <td>0.496416</td>\n      <td>1.000000</td>\n      <td>0.663473</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.258900</td>\n      <td>0.261439</td>\n      <td>0.496416</td>\n      <td>0.496416</td>\n      <td>1.000000</td>\n      <td>0.663473</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.253500</td>\n      <td>0.249988</td>\n      <td>0.496416</td>\n      <td>0.496416</td>\n      <td>1.000000</td>\n      <td>0.663473</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>0.251800</td>\n      <td>0.250443</td>\n      <td>0.496416</td>\n      <td>0.496416</td>\n      <td>1.000000</td>\n      <td>0.663473</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>0.252300</td>\n      <td>0.250063</td>\n      <td>0.496416</td>\n      <td>0.496416</td>\n      <td>1.000000</td>\n      <td>0.663473</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"<ipython-input-5-53ce25c34657>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n<ipython-input-5-53ce25c34657>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n<ipython-input-5-53ce25c34657>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n<ipython-input-5-53ce25c34657>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n<ipython-input-5-53ce25c34657>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n<ipython-input-5-53ce25c34657>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n<ipython-input-5-53ce25c34657>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n<ipython-input-5-53ce25c34657>:44: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn('Was asked to gather along dimension 0, but all '\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"},{"name":"stdout","text":"Predictions saved to 'test_predictions_bert.csv'.\n","output_type":"stream"}],"execution_count":5}]}